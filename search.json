[{"title":"关于硬盘故障预测的研究——[预测模型构建的一点心得]","date":"2021-07-15T07:53:55.000Z","url":"/2021/07/15/%E5%85%B3%E4%BA%8E%E7%A1%AC%E7%9B%98%E6%95%85%E9%9A%9C%E9%A2%84%E6%B5%8B%E7%9A%84%E7%A0%94%E7%A9%B6%E2%80%94%E2%80%94-%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E7%9A%84%E4%B8%80%E7%82%B9%E5%BF%83%E5%BE%97/","categories":[["undefined",""]],"content":"前言&emsp;&emsp;研究生阶段入坑了硬盘故障预测的相关研究，并在导师与博士学长的引导下参与了几个大大小小的相关项目，这里记录一下学习心得，并提出一些自己的疑问，在留给自己以后温习的同时也希望能对看到这篇博客的人有些许帮助，不足之处欢迎不吝赐教，疑问处若有共鸣也希望能讨论交流！ 现状&emsp;&emsp;现在绝大部分关于硬盘故障预测的研究都或多或少使用了人工智能的技术，包括： 传统机器学习方法，在我读过的大部分论文中， RandomForest（随机森林）都有很好的表现，也是我当前常用的基础模型。 神经网络，如 LSTM 等，这部分我涉猎很少，就不献丑了。 特殊的神经网络模型，如 GAN，这也是我下一步准备的一个研究方向，目前已有研究者通过使用 GAN 巧妙地解决硬盘故障预测中的“冷启动”问题，我觉得这在实际的生产环境中有较高的应用价值，关于这点我会专门写一篇博客进行讨论并验证其实用性，当然啥时候能写出来也完全取决于我的研究进度了。。。。 &emsp;&emsp;既然使用了人工智能模型，那必然会有相应的数据集。目前来说，最常用的、且与硬盘故障强相关的特征属性就是SMART。当然也不断有研究者提出新的与硬盘故障相关的特征属性，如硬盘故障的时空局域性等等，这里不再赘述。 &emsp;&emsp;这篇文章主要基于传统机器学习方法与硬盘的 SMART 属性来展开，演示模型为 RandomForest，数据集使用开源数据集 BackBlaze。 数据预处理与模型调优的关系&emsp;&emsp;常用的数据预处理手段包括数据清洗、样本均衡、标准化等，面对一些非数值型的数据，如Firmware，还需要进行数据编码，常用的编码方式有标签编码、独热编码等。此外，还可以通过计算获得新的特征列。例如计算某个属性的增量、计算两个属性的比值等，某些通过计算得到的新属性也对故障预测有很大帮助，打个比方：盘的坏块会随着使用时间增加而增加，当某一时刻坏块的数量激增，这就可能意味着硬盘即将损坏。 &emsp;&emsp;那么数据预处理与模型调优又有什么关系呢？在实验过程中我发现，常规的调优手段（如交叉验证、网格搜索等）对模型预测效果的影响十分有限，反而训练样本的一些属性对模型的性能影响更大，这里首先介绍几个概念： 训练集、测试集：训练集用来训练模型、测试集用来测试模型的性能。这里面还会涉及到验证集的概念，验证集用来选择最佳的超参数组合，与交叉验证也有关，这里不再赘述。 正样本、负样本：这里的正样本指故障盘的数据样本、负样本则是正常盘数据样本。 预测窗口：这个概念十分重要，它不仅可以帮助扩充训练数据集的大小，对模型性能的影响也很大。 &emsp;&emsp;从图中不难看出，随着硬盘的使用，硬盘会不断趋向故障。于是提出预测窗口这个概念，在预测窗口内的数据均视为故障数据、预测窗口前的数据均视为正常数据，这样做还可以一定程度上扩充故障样本数量。下面将会展示部分实验数据解释这些样本属性与模型性能调优的关系。 &emsp;&emsp;首先是分析正、负样本比例与模型性能的关系，这里分为3组进行实验，每一组的预测窗口保持一致，正负样本比例取1:1、1:2、1:3、1:5，主要考量召回率、误报率，其中，横坐标的命名方式为w_p_n，w为预测窗口大小，p为正样本比例、n为负样本比例： &emsp;&emsp;从图中不难发现，随着正负样本比例变大（即正样本占比增加），模型的召回率逐渐上升，模型的误报率也逐渐上升，但是上升的幅度都比较小。 &emsp;&emsp;然后分析预测窗口大小与模型性能的关系，这里分为4组实验进行讨论，每组的正负样本比例不变，预测窗口分别取1、7、14： &emsp;&emsp;从图中可以看出，仅用故障当天的数据作为故障盘样本会导致模型的性能偏低，导致这一现象的原因可能是因为故障盘预测窗口=1时，故障样本较少，模型无法充分学习故障样本故障样本的特征，从而导致召回率很差。当然预测窗口也不是越大越好，因为预测窗口大小为7和14时的性能差距并不明显，而且样本越多，模型训练也会越慢。 &emsp;&emsp;此外，从这些实验中不难看出召回率与误报率是两个矛盾的指标，很难在提高召回率的同时还能降低误报率。加长预测窗口似乎有效，但其带来的收益也比较有限，所以需要折中选择最合适的样本处理方案。不过也有研究者通过多模型的投票机制降低了甚至避免了误报，但这会带来相当大的模型训练与运行开销。 关于模型有效性的思考&emsp;&emsp;硬盘故障预测是一种有相当风险的技术，不能百分百依赖故障预测的结果，数据冗余备份和恢复等手段是不可或缺的。在实验过程中我发现，尽管模型在训练的过程中表现良好（达到95%以上的召回率），但在实际预测过程中表现却很差（召回率低于60%）。例如，我们使用1月~11月的数据训练模型并达到95%的召回率，然后去预测12月的数据。这时召回率会骤降30%左右。根据现有经验分析其可能的原因： 模型训练时发生过拟合，但这种可能性很小。 模型训练未考虑到数据样本的时序性。因为例如SMART之类的特征，他的值是随时间递增的，在使用传统方法训练模型时，我们经常会对样本进行随机分割（train_test_split等）、随机抽样（smote等），这样会打乱样本的时序，造成这样一种结果：使用未来数据训练模型，用历史数据测试、调整模型。这样训练出来的模型当然对未来数据的预测效果很差。后面我会专门讨论基于时序的模型训练，并给出相应的实验数据。 可能还有其他原因，后续再补充。 &emsp;&emsp;模型的有效性是一个值得深究的问题，我们当然可以仅通过传统的训练方式得到非常优秀的实验数据，但从实际应用的角度来说毫无意义，而且也经不起的推敲。 就先到这。。。"},{"title":"关于故障硬盘数据迁移的粗略研究——硬盘信息采集","date":"2021-07-06T04:07:13.000Z","url":"/2021/07/06/%E5%85%B3%E4%BA%8E%E6%95%85%E9%9A%9C%E7%A1%AC%E7%9B%98%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB%E7%9A%84%E7%B2%97%E7%95%A5%E7%A0%94%E7%A9%B6%E2%80%94%E2%80%94%E7%A1%AC%E7%9B%98%E4%BF%A1%E6%81%AF%E9%87%87%E9%9B%86/","categories":[["undefined",""]],"content":"简介&emsp;&emsp;在该项目中，我们提供了集群监控、硬盘故障预测的实现，甲方进一步提出要能在预测出故障后方便地进行磁盘数据转移。由于之前没研究过数据迁移和故障恢复，所以只能从头慢慢摸索。目前的大致思路就是：将源硬盘（快故障的硬盘）的数据进行压缩；将目的硬盘（正常硬盘）格式化并进行分区；在源节点与目的节点之间进行“点对点”的数据传输；最后在目的硬盘上恢复数据。项目地址在这里。 &emsp;&emsp;目前仅在 Windows 系统上进行了实现，Linux 系统上由于 dd 、scp 等命令的存在，可能实现起来更加简单。 实现步骤一、在 Java 程序中调用 Windows 系统命令 执行系统命令 获取返回信息 executeCmd() 在后面会经常用到。 二、通过 PowerShell 的 Get-WmiObject 命令获取硬盘信息 什么是 Wmi ​ WMI就是 Windows Management Instrumentation（Windows 管理规范）。它是 Windows 中的一个核心管理技术。WMI 为访问大量的 Windows 管理数据和方法的提供了一个统一的机制。WMI通过脚本、C++程序接口、.NET类（系统管理）和命令行工具（WMIC）提供了对这个信息的访问。WMI的功能还包括事件、远程、查询、查看、计划和实施用户扩展及更多内容。 例如保存硬盘分区信息的 Wmi 类如下：（详细可查询） 通过 PowerShell 命令获取Wmi信息 这里要使用到的命令为 Get-WmiObject，命令格式为： 我自己封装的获取硬盘信息的工具类如下： 至此，我们已经能获取到硬盘的基本信息了，在后续的工作中，需要基于这些信息进行硬盘数据转移工作，包括硬盘分区、数据传输、数据恢复等，后续文章会一一介绍。 "},{"title":"使用flask+html实现的个人用数据分析程序","date":"2021-07-05T07:09:59.000Z","url":"/2021/07/05/%E4%BD%BF%E7%94%A8flask-html%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%B8%AA%E4%BA%BA%E7%94%A8%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%A8%8B%E5%BA%8F/","categories":[["undefined",""]],"content":"简介&emsp;&emsp;由于做故障预测的相关研究经常要分析原始数据，每次都要写代码比较麻烦，所以考虑用数据分析软件来做。但目前市面上的数据分析软件功能普遍比较复杂且不符合要求，所以打算自己定制一个数据分析程序，供以后研究学习使用。根据自己的需求可以随时追加各种数据分析方法。详细代码见：Ruabit18/Data_Analyze: 数据分析 (github.com) 程序的整体框架图如下： 程序的运行界面如下： 用到的编程语言有： Python（3.9）：负责将原始数据入库、提供数据访问接口、提供数据分析方法。使用的库主要有Sqlite3、Pandas、Flask。 HTML5 + JS + CSS：这部分负责数据的展示、提供操作面板。主要用到了ECharts图表插件。 实现步骤一、数据访问层——SQLite SQLite&emsp;&emsp;SQLite是一种轻型数据库，是遵守ACID的关系型数据库管理系统。它的特点是数据库管理程序集成在代码中，不需要安装额外的程序，数据库与数据表均以文件的形式保存在一起。同时也有相应的可视化工具来方便管理，如SQLite Studio等。 数据库引用与创建 数据库语句执行 这里以数据库查询为例 数据入库原始数据来自BackBlaze采集的硬盘故障数据，文件格式为.csv，数据格式如下： 将全部csv文件导入数据库： 二、业务逻辑层——Flask Flask&emsp;&emsp;Flask是一个使用 Python 编写的轻量级 Web 应用框架，使用Flask能很方便的实现前端与后台的交互流程，这里将 Flask 框架用于本地应用程序。详细资料参考Flask 中文文档 (2.0.1)。 使用Flask实现简单业务逻辑 初始引用及配置 创建一个微服务程序，类似于 Java 的 Servlet 启动 Flask 后台 举一个简单的请求微服务的例子 三、表示层——HTML5​ 前端编程我只是略懂，稍微会做点简单的网页，这里就不献丑了，列举几个常用的网站： Palettable：按照喜好生成不同的定制配色方案 Color Hunt：很多配色方案可供选择 Apache ECharts：十分好用的图表插件，只需要按照JSON格式进行配置就能生成漂亮的图表，并有丰富的交互功能，还可在线编辑预览Example "}]